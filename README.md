# Timothy Mosby - XR Engineer & Patent Inventor

> Building the future of spatial computing through gesture-based interfaces and immersive experiences

## ğŸš€ About Me

Seasoned XR engineer with 15+ years building touch interfaces, from Microsoft Surface to cross-platform mobile applications. Patent inventor with **146+ industry citations** from Apple, Microsoft, Amazon, Google, and Samsung. Currently developing immersive applications for next-generation platforms with a passion for shaping the intersection where digital and physical worlds collide.

**Career Progression:** Gesture Computing â†’ Web Platforms â†’ Mobile Apps â†’ XR/Spatial Computing

## ğŸ† Patent & Innovation

### US8619036B2 - Virtual keyboard based activation and dismissal (2013)

Intuitive virtual keyboard layout technology that adapts position and orientation based on natural hand placement, positioning the home row directly beneath the user's fingertips.

- **146+ forward citations** across AR/VR spatial computing, touch interfaces, and gesture recognition
- **Cited by major tech companies:** Apple, Microsoft, Amazon, Google, Samsung
- **Foundational gesture detection technology** enabling modern touch and spatial input systems

## ğŸ’» XR Projects Portfolio

### 1. ğŸ¤² Hand Tracking Sign Language App

**Status:** Current Development  
**Tech:** Unity, OpenXR, Meta Quest 3, Hand Tracking, C#

XR application teaching baby sign language through hand-tracked interactions. Built with Unity, OpenXR, and Meta Quest 3, combining foundational gesture detection expertise with accessible education technology.

### 2. ğŸ‘ï¸ OpenCV Gesture Recognition Library

**Status:** Proposed  
**Tech:** OpenCV, Computer Vision, Unity SDK, GitHub, Machine Learning

Open-source computer vision toolkit for XR developers, implementing gesture detection algorithms inspired by foundational keyboard positioning research. Published on GitHub with Unity integration examples.

### 3. ğŸ¥½ XREAL One Pro Spatial Computing Study

**Status:** Proposed  
**Tech:** XREAL SDK 3.0, Unity XR, AR Foundation, Applied Research, UX Study

Comparative analysis of lightweight AR glasses vs. headset-based spatial computing. Exploring hand tracking accuracy, FOV limitations, and social acceptability of glasses-form-factor AR using XREAL SDK 3.0 and Unity XR Integration.

### 4. ğŸ”„ Cross-Platform AR Gesture Framework

**Status:** Proposed  
**Tech:** Cross-Platform, ARKit, Meta XR SDK, XREAL SDK, Gesture Recognition

Multi-device gesture recognition system supporting XREAL One Pro, Meta Quest, and iOS ARKit. Investigating gesture consistency across different tracking systems and form factors with published compatibility matrix.

### 5. ğŸ‘¥ Lightweight AR Collaboration Prototype

**Status:** Proposed  
**Tech:** Spatial Anchors, Multi-User AR, XREAL SDK, Collaboration, Social Computing

XREAL-based shared workspace application exploring persistent spatial anchors and multi-user hand tracking. Compares social AR interaction patterns between glasses vs. headset form factors.

## ğŸ› ï¸ Technical Skills

**XR & Spatial Computing:** Spatial Computing, Hand Tracking, Unity, OpenXR, XR Hands, Unity XR Interaction Toolkit, Meta XR SDK

**Core Languages:** C++, C#, JavaScript, Kotlin, Swift

**Platforms & Tools:** .NET, Xcode, Android Studio, Git, CI/CD, REST APIs, Google Cloud, Firebase, TDD, Agile

## ğŸ… Awards

**Getty Images Hackathon 1st Place â€¢ 01/2017**  
**Solo-developed AR wall art implementation** using ArgonJS for cross-platform mobile browsers, **months before Apple ARKit** and **Google ARCore released**. Part of winning team with shared UX and ideation phase, delivering dual implementations: true spatial AR via ArgonJS and iOS native using Swift.

## ğŸ“ Contact

- **Email:** timothy.mosby@gmail.com
- **Phone:** (206) 650-0141
- **LinkedIn:** [linkedin.com/in/timothymosby](https://linkedin.com/in/timothymosby)
- **Location:** Bremerton, Washington, US

---

### ğŸŒŸ Ready to discuss XR innovation and spatial computing opportunities?

_Currently seeking senior XR engineering roles where I can apply my foundational gesture detection expertise to build next-generation immersive experiences._
